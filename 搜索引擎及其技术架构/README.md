## 介绍： 
* 三代
	1. 文本检索
	2. 链接分析
    3. 用户中心
* 目标：更全，更快，**更准[关键]**

## 网络爬虫：
* 精心选择一部分网页作为种子URL
* 注意去重和反作弊
* 爬虫类型
	1. 批量型：只抓一定数量页面
    2. **增量型**：一直抓取，并且定期更新内容
    3. 垂直型：只关注特定领域
* 优秀爬虫的特性：
	1. 高性能：网页抓取速度
    2. 可拓展性：可通过增加服务器和爬虫数目来增加性能 
	    * 分布式
	    * 多线程
	    * 并发性
  3. 健壮性： 
	  * 遭遇html不规范
	  * 被抓取服务器宕机
	  * 爬虫陷阱
	  * 爬虫挂了重启后不必重头抓起
  4. 友好型： 
	  * 保护网站私密
	      * 爬虫禁抓协议robot.txt
	      * 网页紧抓标记：noindex/nofollow
	  * 不增加被抓去网站负载->类似DOS攻击
* 爬虫质量评价标准：
	* 抓取网页覆盖率
	* 抓取网页时新性
	* 抓取网页重要性->搜索精度
	* 例子：谷歌，两套不同目的的爬虫系统：
	    1. Fresh Bot：考虑时新性【甚至秒抓】
	    2. Deep Crawl Bot：一天为周期来抓不怎么更新的
* 抓取策略：
	* 目标：优先选择重要的
	* 方案：
	    1. 宽度有限 Breath First
	    2. 非完全PageRank策略 Partial PageRank(不一定比BF方法好)： 
	        * 对待抓取队列使用PR->判断队列URL重要性
	        * 攒够K个页面在计算一次PR->提高效率
	        * 根据已抓取页面的链接指向次数，赋予新抓取的网页赋予临时PR
	    3. 在线页面重要性计算 OCIP(Online Page Importance Compution) 
	        * 初始化：每个互联网页面都有相同的权重
		    * 迭代：每次抓取一个新页面后，将自己权重平分给包含的链接的页面，自己权重清空
		    * 优先爬取当前权重最高的页面
		    * 实验表明略优于BF方法
	    4. 大站优先：实验表明略优于BF方法
* 网页更新策略：
	* 历史参考策略：泊松过程来建模（主要研究核心内容变化，而不是广告栏之类的变化）
	* 用户体验策略：保存多个历史版本，根据每个版本对搜索质量的影响来判断
    * 聚类抽样策略：根据同类网站(依据：网站表现特征)更新频率来判断
	    * 静态特征:
		    * 内容
		    * 图片数量
		    * 页面大小
		    * 链接深度
		    * PR
      * 动态特征：
	      * 图片数量变化
	      * 入链出链数目变化
* 暗网抓取：
	* 将暗网数据从数据库中挖掘出来，加入搜索引擎索引
	* 需要模拟人的操作提交表单
	* 难点：
	    1. 组合项： 
	        * 组合总类太多，给网站造成压力
	        * 解决方法：Google提出“富含信息查询模板”（Informative Query Templates）  
选择其中几个属性组合去提交，如果返回来的内容大多一样，那么说明不是富含信息查询模板
	        * 进一步改进：ISIT算法  
              从一维模板开始逐个查询，直到无法找出富含信息查询模板为止
	    2. 文本框填写：  
           人工提供种子词表，然后爬回来数据自动挖掘关键词
* 分布式爬虫：
	* 主从式：
		* 主：URL分发，做好从服务器负载均衡，性能是系统的瓶颈
	    * 从：实际下载网页，从服务器之间无通信
    * 对等式：每台服务器根据**主域名**的哈希值来判断是不是由自己来抓取，不是就将网址发给相应服务器  
改进：一致性哈希算法：对于不属于自己抓取的网址转发URL是如果那部机子宕机了，就找到相应的下一个负责的机器
* 倒排索引：
	1. 文件编号
	2. 词频
	3. 文档集合包含这个词的频率
	4. 出现位置
* 存储词典：
	* 哈希+链表
	* B/B+树
* 建立索引：
	* 两遍法：第一遍找出需要的信息以及计算所需内存，第二遍写入。内存消耗大
	* 排序法：分配固定内存，用完就写到硬盘中。排序是为了方便合并中间文件。
	* 归并法：每次都把中间结果全部写进磁盘，消除了排序法越到后面写入词项越少的弊端。
* 动态索引：


